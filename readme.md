# Project: cloud-etl-modernization-airflow-aws

## ğŸš€ Overview
This project modernizes a legacy ETL pipeline using **Apache Airflow**, **AWS**, and **Python**. It demonstrates how to automate data ingestion from third-party APIs, transform the data, and store it in a cloud-native architecture (S3, Redshift). Inspired by a real-world pipeline refactor that reduced processing time by 40% and enabled real-time reporting across business teams.

---

## ğŸ§± Architecture
**Source**: Mock Ads API / JSON files  
**Orchestration**: Apache Airflow  
**Storage**: AWS S3 (raw layer), optionally Redshift (modeled layer)  
**Transformation**: PySpark / Pandas  
**Deployment**: GitHub Codespaces, Docker, CI/CD with GitHub Actions

---

## ğŸ“‚ Directory Structure
```
cloud-etl-modernization-airflow-aws/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ api_ingestion_dag.py          # DAG to ingest ads data into S3
â”‚   â”œâ”€â”€ data_cleanup_dag.py           # (to be implemented)
â”‚   â””â”€â”€ model_refresh_dag.py          # (to be implemented)
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ check_file_trigger.py         # (future: file-based Lambda trigger)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ transform_ads_data.py         # Transforms raw ads data into KPIs
â”‚   â””â”€â”€ load_to_redshift.py           # (future: Redshift load script)
â”œâ”€â”€ mock_data/
â”‚   â””â”€â”€ ads_sample.json               # Sample mock input for local dev
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ connections.json              # Airflow connection templates (local)
â”œâ”€â”€ docker-compose.yml                # Local Airflow dev environment
â”œâ”€â”€ .github/workflows/ci-pipeline.yml# CI/CD pipeline via GitHub Actions
â”œâ”€â”€ README.md                         # Youâ€™re here
```

---

## âš™ï¸ Features
- âœ… Modular Airflow DAGs for ingestion, transformation, and loading
- âœ… Serverless ingestion with AWS Lambda & S3
- âœ… Automated transformation using Pandas
- âœ… Configurable to run locally or in the cloud
- âœ… Version-controlled in GitHub Codespaces with CI/CD support

---

## ğŸ§ª How to Run Locally
```bash
# Step 1: Clone the repo
$ git clone https://github.com/yourusername/cloud-etl-modernization-airflow-aws

# Step 2: Launch Airflow locally (requires Docker)
$ docker-compose up -d

# Step 3: Trigger the DAG manually from the Airflow UI
# (http://localhost:8080)
```

---

## ğŸ“Š Example Output
Sample output from `transform_ads_data.py`:
```csv
campaign_id,impressions,clicks,spend,date,ctr
abc123,10000,345,123.45,2024-06-01,0.0345
```

---

## ğŸ‘¤ Author
**Bita Ashoori**  
GitHub: [bashoori](https://github.com/bashoori)  

---

## ğŸ“œ License
MIT License. Feel free to fork, improve, and contribute!
