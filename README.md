# ğŸŒ©ï¸ Cloud ETL Modernization with Apache Airflow & AWS

An end-to-end **ETL pipeline** project using **Apache Airflow** for workflow orchestration and **AWS (Redshift, S3)** for scalable cloud storage and data loading. This project simulates a modern, production-ready data engineering environment that ingests mock API data, transforms it, and loads it into a cloud data warehouse.

---

## ğŸ“Œ Purpose

This project showcases:

- How to orchestrate real-world ETL workflows using Airflow
- Modular task design with Python operators
- Cloud integration with AWS S3 and Redshift
- Local development inside GitHub Codespaces using Docker and DevContainers

---

## âš™ï¸ Architecture Overview

```text
         +------------+        +------------------+       +------------------+
         | Mock API   | -----> | Airflow DAG:     | ----> | transform_ads.py |
         | (JSON Ads) |        | fetch_ads_data   |       +------------------+
         +------------+                                     |
                                                             v
                                                      +------------------+
                                                      | load_to_redshift |
                                                      +------------------+
                                                             |
                                                             v
                                                    +--------------------+
                                                    | AWS Redshift Table |
                                                    +--------------------+

ğŸš€ Features
	â€¢	ğŸŒ€ Modular DAGs: fetch_ads_data, transform_ads_data, load_to_redshift
	â€¢	ğŸ³ Dockerized with Airflow 2.7 for easy reproducibility
	â€¢	ğŸ§ª Devcontainer-ready for GitHub Codespaces
	â€¢	ğŸ“¥ Mock API input via local files (mock_data)
	â€¢	ğŸ§¹ Transformation logic via Python scripts
	â€¢	ğŸ”º Cloud Output to AWS Redshift (simulated/optional)



## ğŸ§° Tech Stack

This project is powered by modern, cloud-friendly, and modular tools built for scalable data workflows.

| Category                | Tools & Technologies                                                  |
|-------------------------|-----------------------------------------------------------------------|
| **Language**            | Python 3.9                                                            |
| **Orchestration**       | Apache Airflow (v2.7.3)                                               |
| **Environment**         | Docker + DevContainer + GitHub Codespaces                            |
| **ETL Scripting**       | Custom Python scripts (`/scripts`) for ingestion, transformation     |
| **Storage (Mocked)**    | AWS S3 (emulated via local directory), AWS Redshift (via placeholder) |
| **Data Format**         | JSON (mock API), CSV (transformed), Pandas DataFrames                |
| **Scheduler**           | Airflow's built-in scheduler & CLI-triggered DAGs                    |
| **Development**         | VS Code + Remote Containers + GitHub                                |

---

âœ… Designed for clarity, portability, and educational purposes.  
ğŸ“¦ Easily extensible to real cloud infrastructure (S3, Redshift, RDS, etc.).


cloud-etl-modernization-airflow-aws/
â”‚
â”œâ”€â”€ .devcontainer/                # DevContainer setup for Codespaces
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ devcontainer.json
â”‚
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ api_ingestion_dag.py
â”‚
â”œâ”€â”€ mock_data/                    # Simulated API data
â”‚   â””â”€â”€ ads_data.json
â”‚
â”œâ”€â”€ scripts/                      # ETL scripts
â”‚   â”œâ”€â”€ transform_ads_data.py
â”‚   â””â”€â”€ load_to_redshift.py
â”‚
â”œâ”€â”€ docker-compose.yml           # Airflow service definitions
â””â”€â”€ README.md

âš™ï¸ How to Run (Locally or in Codespaces)

âœ… Prerequisites
	â€¢	GitHub Codespaces OR Docker & Python 3.9 locally
	â€¢	AWS credentials (if testing with real Redshift)
	â€¢	Replace Redshift config in load_to_redshift.py if connecting to real DB

â–¶ï¸ Local Setup
# Clone this repo
git clone https://github.com/bashoori/cloud-etl-modernization-airflow-aws
cd cloud-etl-modernization-airflow-aws

# Launch Airflow using Docker Compose
docker-compose up --build

Then visit: http://localhost:8080
	â€¢	Default credentials: admin / admin (or set via Airflow CLI)

ğŸ§ª Airflow UI (Live Demo in Codespaces)

The project auto-starts:
	â€¢	Airflow scheduler
	â€¢	Airflow webserver
	â€¢	Database migration
	â€¢	Admin user creation

No setup needed! Just open Port 8080 from your Codespace.

â¸»



# ğŸ“Œ Notable Airflow DAGs

This project includes modular Airflow DAGs designed for modern ETL workflows. Each DAG is focused on a specific stage of the data pipeline, from data ingestion to transformation and loading.

## DAG: `api_ingestion_dag`

An end-to-end DAG that orchestrates the entire pipeline:
1. **Ingests** mock advertisement data (JSON) from a local mock source
2. **Transforms** raw JSON into structured tabular format
3. **Loads** cleaned data into AWS Redshift (or mock DB in development)

### ğŸ“‚ Tasks Breakdown:

| Task ID            | Description                                | Operator       |
|--------------------|--------------------------------------------|----------------|
| `fetch_ads_data`   | Reads and parses JSON ad data              | PythonOperator |
| `transform_ads`    | Cleans and structures the ads data         | PythonOperator |
| `load_to_redshift` | Writes the transformed data to Redshift    | PythonOperator |

### âš™ï¸ DAG Config:

- **Schedule**: `@daily`
- **Retries**: `1` (with delay)
- **Owner**: `bita`
- **Dependencies**: `transform_ads` depends on `fetch_ads_data`; `load_to_redshift` runs last.

---

## ğŸ’¡ DAG Highlights

- Modular and readable â€” easy to expand for other datasets
- Local JSON ingestion simulates external API
- Transformation and loading logic is abstracted into Python scripts in `/scripts`
- Fully reproducible using Docker and GitHub Codespaces

---

## ğŸ§ª How to Use in Airflow

Once Airflow is running (via Docker or Codespaces), the DAG should appear automatically in the UI.  
Activate it and trigger manually or wait for the daily schedule.

---

> âœ¨ This DAG is part of the [cloud-etl-modernization-airflow-aws](https://github.com/bashoori/cloud-etl-modernization-airflow-aws) project.



âœ¨ Highlights
	â€¢	Runs on Docker in VS Code + Codespaces
	â€¢	Hands-on Airflow DAG design
	â€¢	Modular structure â€” easy to plug in APIs, cloud storage, DBs
	â€¢	Great starting point for cloud-native ETL projects

â¸»

ğŸ“ˆ Future Improvements
	â€¢	Connect to real APIs like Facebook Ads or Google Analytics
	â€¢	Replace SQLite with PostgreSQL for dev DB
	â€¢	Add automated tests and monitoring DAGs
	â€¢	Enable dbt for advanced transformations

â¸»

ğŸ™‹â€â™€ï¸ Author

Bita Ashoori
Connect on LinkedIn
GitHub: bashoori

â¸»

ğŸ’¡ This project is part of my Data Engineering Portfolio.
