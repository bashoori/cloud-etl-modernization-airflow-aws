# ğŸŒ©ï¸ Cloud ETL Modernization with Apache Airflow & AWS

An end-to-end **ETL pipeline** project using **Apache Airflow** for workflow orchestration and **AWS (Redshift, S3)** for scalable cloud storage and data loading.  
It simulates a modern, production-ready data engineering environment for ingesting mock API data, transforming it, and loading it into a cloud data warehouse.

---

## ğŸ“Œ Purpose

- Demonstrate orchestrated ETL pipelines using Airflow
- Modular task design with Python
- Cloud integration with AWS S3 & Redshift (mocked)
- DevContainer + Codespaces for portable, reproducible dev

---

## âš™ï¸ Architecture Overview

```text
+------------+       +---------------------+       +---------------------+
|  Mock API  | --->  |  fetch_ads_data     | --->  |  transform_ads.py   |
| (JSON Ads) |       |  (Airflow DAG Task) |       | (Transforms to CSV) |
+------------+       +---------------------+       +---------------------+
                                                 |
                                                 v
                                       +-------------------------+
                                       |  load_to_redshift.py    |
                                       | (Load to Redshift Table)|
                                       +-------------------------+
                                                 |
                                                 v
                                       +-------------------------+
                                       |   AWS Redshift Table    |
                                       +-------------------------+

## ğŸš€ Features

- ğŸŒ€ Modular DAGs: `fetch_ads_data`, `transform_ads`, `load_to_redshift`
- ğŸ³ Dockerized with Airflow 2.7 for easy reproducibility
- ğŸ§ª Devcontainer-ready for GitHub Codespaces
- ğŸ“¥ Mock API input via local files (`mock_data`)
- ğŸ”„ Python-based transformations
- ğŸ”º Simulated Redshift loading

---

## ğŸ§° Tech Stack

| Category              | Tools & Technologies                                                  |
|-----------------------|-----------------------------------------------------------------------|
| **Language**          | Python 3.9                                                            |
| **Orchestration**     | Apache Airflow (v2.7.3)                                               |
| **Environment**       | Docker, DevContainers, GitHub Codespaces                             |
| **ETL Scripting**     | Python scripts (`/scripts`)                                           |
| **Storage (Mocked)**  | AWS S3 (local dir), AWS Redshift (simulated)                         |
| **Data Format**       | JSON input, CSV output, Pandas DataFrames                            |
| **Dev Tooling**       | VS Code + Remote Containers                                           |

âœ… Built for clarity and learning  
ğŸ“¦ Ready to adapt to real infrastructure (S3, RDS, Redshift, etc.)

---

## ğŸ“ Project Structure
cloud-etl-modernization-airflow-aws/
â”‚
â”œâ”€â”€ .devcontainer/                # Devcontainer setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ devcontainer.json
â”‚
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ api_ingestion_dag.py
â”‚
â”œâ”€â”€ mock_data/                    # Mock JSON input
â”‚   â””â”€â”€ ads_data.json
â”‚
â”œâ”€â”€ scripts/                      # ETL logic
â”‚   â”œâ”€â”€ transform_ads_data.py
â”‚   â””â”€â”€ load_to_redshift.py
â”‚
â”œâ”€â”€ docker-compose.yml           # Airflow stack
â””â”€â”€ README.md

---

## â–¶ï¸ How to Run

### âœ… Prerequisites

- GitHub Codespaces **or** local Docker
- Python 3.9+
- AWS Redshift creds (optional)

### ğŸš€ Local Setup

```bash
git clone https://github.com/bashoori/cloud-etl-modernization-airflow-aws
cd cloud-etl-modernization-airflow-aws
docker-compose up --build

Then visit: http://localhost:8080
â†’ Default login: admin / admin

â¸»

ğŸ§ª Codespaces Experience
	â€¢	Starts the Airflow scheduler & webserver automatically
	â€¢	Database migration & admin user setup is handled in postCreateCommand
	â€¢	Open port 8080 to access the UI instantly

â¸»

ğŸ“Œ Notable Airflow DAGs

### `api_ingestion_dag.py`

Orchestrates the ETL flow:

| **Task ID**         | **Description**                         | **Operator**     |
|---------------------|-----------------------------------------|------------------|
| `fetch_ads_data`    | Reads JSON ad data                      | PythonOperator   |
| `transform_ads`     | Cleans/structures data                  | PythonOperator   |
| `load_to_redshift`  | Loads to Redshift or mock destination   | PythonOperator   |

- â° **Schedule**: `@daily`  
- ğŸ” **Retries**: `1`  
- ğŸ‘¤ **Owner**: `bita`



ğŸ’¡ Highlights
	â€¢	Modular, reusable DAG components
	â€¢	Easy to extend with real cloud infra (APIs, Redshift)
	â€¢	Run locally or in the cloud instantly via Codespaces
	â€¢	Clear separation of ingestion, transform, load steps

â¸»

ğŸ“ˆ Future Improvements
	â€¢	Use real external APIs (Facebook Ads, GA4)
	â€¢	Swap SQLite for PostgreSQL in dev
	â€¢	Add testing suite for DAGs and scripts
	â€¢	Add dbt-based transformations

â¸»

ğŸ™‹â€â™€ï¸ Author

Bita Ashoori
GitHub ãƒ» LinkedIn

ğŸ’¼ Part of my Data Engineering Portfolio
